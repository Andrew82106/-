{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "生成数据集"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features: tensor([-0.7881,  2.5178]) \n",
      "label: tensor([-5.9475])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils import data\n",
    "from d2l import torch as d2l\n",
    "def synthetic_data(w, b, num_examples):  #@save\n",
    "    \"\"\"生成X和y，其中的关系为y=Xw+b+噪声，w=[2,-3.4]^T\"\"\"\n",
    "    X = torch.normal(0, 1, (num_examples, len(w)))\n",
    "    # 产生一个服从N(0,1)，大小为num_examples X len(w)的正态分布矩阵\n",
    "    # 这里服从正态分布的意思是每一个数是从正态分布中等可能取出的，而不是整个矩阵呈现正态分布的样子\n",
    "    y = torch.matmul(X, w) + b\n",
    "    # 计算矩阵向量积Xw，然后加上b\n",
    "    y += torch.normal(0, 0.01, y.shape)\n",
    "    return X, y.reshape((-1, 1))\n",
    "    # 最后的y是一个列向量，X是一个1000 x 2的矩阵\n",
    "\n",
    "true_w = torch.tensor([2, -3.4]) # 生成基本数据结构：张量\n",
    "true_b = 4.2\n",
    "# 实际的w和b如上，也就是y=X[2, -3.4] + 4.2\n",
    "features, labels = synthetic_data(true_w, true_b, 1000)\n",
    "# 生成特征和标签\n",
    "print('features:', features[0],'\\nlabel:', labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "读取数据集"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n"
     ]
    },
    {
     "data": {
      "text/plain": "[tensor([[-1.5064, -0.6793],\n         [ 0.2044, -1.5074],\n         [-0.1635,  0.3499],\n         [ 0.5409, -1.7088]]),\n tensor([[ 3.4933],\n         [ 9.7160],\n         [ 2.6644],\n         [11.1012]])]"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def loadDataSets(data_arrays, batch_size, is_train=True):\n",
    "    print(type(data_arrays))\n",
    "    ds = data.TensorDataset(*data_arrays) #星号表示解开list写入其中参数。\n",
    "    return data.DataLoader(dataset=ds, batch_size=batch_size, shuffle=is_train)\n",
    "    # 这里的DataLoader是pytorch里面的可迭代对象。\n",
    "    # 一般的pytorch的数据处理都是使用这三步：\n",
    "    # 1.构造Dataset对象  2.通过DataLoader来构造迭代对象  3.逐步迭代数据\n",
    "\n",
    "batch_size = 4\n",
    "data_iter = loadDataSets((features, labels), batch_size)\n",
    "next(iter(data_iter))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "定义模型"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([0.])"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nn是神经网络的缩写\n",
    "from torch import nn\n",
    "\n",
    "net = nn.Sequential(nn.Linear(2, 1))\n",
    "# 这里的nn.Sequential是一个层容器，里面的参数就是层数。\n",
    "# 这里的nn.Linear(2, 1)就是一个线性层，一个2对1的线性层，本质上就是Y=ax1+bx2\n",
    "net[0].weight.data.normal_(0, 0.01)\n",
    "net[0].bias.data.fill_(0)\n",
    "# 这里是指定每个权重参数应该从均值为0、标准差为0.01的正态分布中随机采样，偏置参数将初始化为零。\n",
    "# 上面的net[0]是访问了Sequential层容器中的第一层模型的参数，也就是线性模型的参数\n",
    "# net[0].weight.data是访问了权重参数，也就对应Linear里面的那个2\n",
    "# net[0].weight.data.normal_(0, 0.01)则是规定权重参数从均值为0、标准差为0.01的正态分布中随机采样\n",
    "# net[0].bias.data.fill_(0)则是将偏置放为0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "定义损失函数"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "loss = nn.MSELoss()\n",
    "# 定义均方误差"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "定义优化算法"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "trainer = torch.optim.SGD(net.parameters(), lr=0.03)\n",
    "# 定义了小批量随机梯度下降算法，其中net.parameters()定义了要优化的参数，lr=0.03是学习率超参数"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "训练"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.000105\n",
      "epoch 2, loss 0.000107\n",
      "epoch 3, loss 0.000104\n",
      "epoch 4, loss 0.000105\n",
      "epoch 5, loss 0.000107\n",
      "epoch 6, loss 0.000108\n",
      "epoch 7, loss 0.000105\n",
      "epoch 8, loss 0.000109\n",
      "epoch 9, loss 0.000105\n",
      "epoch 10, loss 0.000109\n",
      "epoch 11, loss 0.000107\n",
      "epoch 12, loss 0.000110\n",
      "epoch 13, loss 0.000107\n",
      "epoch 14, loss 0.000106\n",
      "epoch 15, loss 0.000104\n",
      "epoch 16, loss 0.000105\n",
      "epoch 17, loss 0.000104\n",
      "epoch 18, loss 0.000107\n",
      "epoch 19, loss 0.000108\n",
      "epoch 20, loss 0.000104\n",
      "epoch 21, loss 0.000104\n",
      "epoch 22, loss 0.000105\n",
      "epoch 23, loss 0.000108\n",
      "epoch 24, loss 0.000105\n",
      "epoch 25, loss 0.000104\n",
      "epoch 26, loss 0.000106\n",
      "epoch 27, loss 0.000109\n",
      "epoch 28, loss 0.000105\n",
      "epoch 29, loss 0.000107\n",
      "epoch 30, loss 0.000106\n",
      "epoch 31, loss 0.000109\n",
      "epoch 32, loss 0.000110\n",
      "epoch 33, loss 0.000106\n",
      "epoch 34, loss 0.000108\n",
      "epoch 35, loss 0.000105\n",
      "epoch 36, loss 0.000105\n",
      "epoch 37, loss 0.000104\n",
      "epoch 38, loss 0.000106\n",
      "epoch 39, loss 0.000106\n",
      "epoch 40, loss 0.000106\n",
      "epoch 41, loss 0.000108\n",
      "epoch 42, loss 0.000108\n",
      "epoch 43, loss 0.000106\n",
      "epoch 44, loss 0.000105\n",
      "epoch 45, loss 0.000111\n",
      "epoch 46, loss 0.000105\n",
      "epoch 47, loss 0.000105\n",
      "epoch 48, loss 0.000107\n",
      "epoch 49, loss 0.000107\n",
      "epoch 50, loss 0.000106\n",
      "epoch 51, loss 0.000105\n",
      "epoch 52, loss 0.000104\n",
      "epoch 53, loss 0.000108\n",
      "epoch 54, loss 0.000106\n",
      "epoch 55, loss 0.000109\n",
      "epoch 56, loss 0.000104\n",
      "epoch 57, loss 0.000105\n",
      "epoch 58, loss 0.000105\n",
      "epoch 59, loss 0.000105\n",
      "epoch 60, loss 0.000105\n",
      "epoch 61, loss 0.000106\n",
      "epoch 62, loss 0.000106\n",
      "epoch 63, loss 0.000105\n",
      "epoch 64, loss 0.000105\n",
      "epoch 65, loss 0.000105\n",
      "epoch 66, loss 0.000105\n",
      "epoch 67, loss 0.000106\n",
      "epoch 68, loss 0.000106\n",
      "epoch 69, loss 0.000108\n",
      "epoch 70, loss 0.000104\n",
      "epoch 71, loss 0.000104\n",
      "epoch 72, loss 0.000109\n",
      "epoch 73, loss 0.000104\n",
      "epoch 74, loss 0.000112\n",
      "epoch 75, loss 0.000107\n",
      "epoch 76, loss 0.000104\n",
      "epoch 77, loss 0.000107\n",
      "epoch 78, loss 0.000105\n",
      "epoch 79, loss 0.000104\n",
      "epoch 80, loss 0.000105\n",
      "epoch 81, loss 0.000104\n",
      "epoch 82, loss 0.000105\n",
      "epoch 83, loss 0.000107\n",
      "epoch 84, loss 0.000104\n",
      "epoch 85, loss 0.000105\n",
      "epoch 86, loss 0.000108\n",
      "epoch 87, loss 0.000104\n",
      "epoch 88, loss 0.000106\n",
      "epoch 89, loss 0.000106\n",
      "epoch 90, loss 0.000110\n",
      "epoch 91, loss 0.000106\n",
      "epoch 92, loss 0.000106\n",
      "epoch 93, loss 0.000104\n",
      "epoch 94, loss 0.000105\n",
      "epoch 95, loss 0.000106\n",
      "epoch 96, loss 0.000107\n",
      "epoch 97, loss 0.000108\n",
      "epoch 98, loss 0.000106\n",
      "epoch 99, loss 0.000105\n",
      "epoch 100, loss 0.000106\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    for X, y in data_iter:\n",
    "        # 每一次迭代data_iter都可以得到一部分样本，这个挺有意思\n",
    "        l = loss(net(X) ,y)\n",
    "        # 计算对于X这一个小样本的误差，后面只对这个小样本进行梯度计算\n",
    "        trainer.zero_grad()\n",
    "        # 梯度清零，否则后续训练梯度会累积\n",
    "        l.backward()\n",
    "        # 反向传播梯度\n",
    "        # 似乎得先反向传播了梯度后才可以更新模型的参数\n",
    "        trainer.step()\n",
    "        # 更新模型的参数\n",
    "    l = loss(net(features), labels)\n",
    "    # 计算误差\n",
    "    print(f'epoch {epoch + 1}, loss {l:f}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w的估计误差： tensor([0.0008, 0.0010])\n",
      "b的估计误差： tensor([-3.2425e-05])\n"
     ]
    }
   ],
   "source": [
    "w = net[0].weight.data\n",
    "print('w的估计误差：', true_w - w.reshape(true_w.shape))\n",
    "b = net[0].bias.data\n",
    "print('b的估计误差：', true_b - b)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}